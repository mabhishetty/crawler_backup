# crawler_backup
A backup repository for my Web crawler

Disclaimer:
I have written this code for non-commercial use only.
The code should not be used for commercial purposes.
Web-crawling can place large demands on web-servers - this should be avoided.
Therefore the code includes a number of measures to crawl responsibly.
These include:
- Respecting robots.txt restrictions
- Reading 'terms of service' sections of websites
- Respecting a crawl-delay if provided. If not, a standard delay shall be used.
The code should not, under any circumstances, be used without these restrictions.
For more information on this, see: https://www.mycustomcrawlerexplanations.com
